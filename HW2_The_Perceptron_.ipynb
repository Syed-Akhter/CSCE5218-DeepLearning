{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "a9d132b6-9530-46c7-c93e-fe78cec4d069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-17 05:50:15--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K)\n",
            "Saving to: ‘test.dat.1’\n",
            "\n",
            "\rtest.dat.1            0%[                    ]       0  --.-KB/s               \rtest.dat.1          100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-17 05:50:15 (318 MB/s) - ‘test.dat.1’ saved [2844/2844]\n",
            "\n",
            "--2023-02-17 05:50:15--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K)\n",
            "Saving to: ‘train.dat.1’\n",
            "\n",
            "train.dat.1         100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-17 05:50:15 (182 MB/s) - ‘train.dat.1’ saved [11244/11244]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the datasets\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ],
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "    data = []\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        instance = [-1] + instance\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    #TODO: Return dot product of array 1 and array 2\n",
        "    sum=0\n",
        "    for i,j in zip(array1,array2):\n",
        "        sum = sum + i*j\n",
        "    return sum \n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    #TODO: Return outpout of sigmoid function on x\n",
        "    return  1/(1+math.exp(-x))\n",
        "\n",
        "\n",
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weights, instance):\n",
        "    #TODO: return the output of the model\n",
        "    return sigmoid(dot_product(weights,instance))\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    #TODO: return the prediction of the model\n",
        "    if(output(weights,instance)>=0.5):\n",
        "        return 1 \n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    # You do not to write code like this, but get used to it\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    #TODO: An array  for weights is created and weights are initialized to 0\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            #TODO: The dot_product is calculated and then the sigmoid of it is claculated i.e., weighted total is passed through an activation function\n",
        "            in_value = dot_product(weights, instance)\n",
        "            output = sigmoid(in_value)\n",
        "            error = instance[-1] - output\n",
        "            #TODO: The weights are adjusted according to the instance\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50YvUza-BYQF",
        "outputId": "63d79b01-d663-469e-ced3-a2616def8042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "\n",
        "In neural networks, inputs are transformed into outputs that can be interpreted as probabilities or predictions using the activation function and dot product. In contrast to the dot product, which performs a linear transformation of the input, the activation function adds non-linearity and converts the output into a probability or a prediction. By integrating these two methods, the neural network may discover complex non-linear relationships between input and output. T In contrast, the predict function of a neural network would only provide a single output value based on the input and weights and  is unable to express the complex relationships between input and output that a neural network is designed to learn because it lacks the adaptability and non-linearity offered by the activation and dot product functions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-VKJOUu2BTp",
        "outputId": "f4daf1a1-0ea0-49a1-8353-e94e41ed0613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "    for tr_size in tr_percent:\n",
        "        for epochs in num_epochs:\n",
        "            size =  round(len(instances_tr)*tr_size/100)\n",
        "            pre_instances = instances_tr[0:size]\n",
        "            weights = train_perceptron(pre_instances, lr, epochs)\n",
        "            accuracy = get_accuracy(weights, instances_te)\n",
        "            print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n",
        "A. It is not necessary to train with the complete training dataset in order to achieve the best accuracy on the test dataset. Using all of the available data may not necessarily result in an increase in the model's accuracy. The performance of a model depends on the quality of the data rather than the quantity. So, it's essential to check that the training set includes all possible data variations and is sufficiently diverse. \n",
        "\n",
        "B. The second one's lower accuracy can be attributed to the model's lower learning rate leading to a delayed convergence and less accurate findings. As can be seen from the outputs, it's probable that the first run's hyperparameters were more appropriate for the circumstance and that the second run's hyperparameters need to be further changed.\n",
        "\n",
        "C. Adding more hyperparameters may or may not improve the accuracy of the model , but a hyperparameter tuning based on the data maybe able to provide the more accuracy , basically depending on the type of data and relation between the said data. Inclusion of inappropriate hyperparameters can also reduce the accuracy of model.\n",
        "\n",
        "D. It can lead to overfitting and the model may perform well in the train set but may underperform in the test sets. This may be the result of the model being perfectly matched to the given data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38rA_Kp3wiBX"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}